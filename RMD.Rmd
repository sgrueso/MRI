---
title: "Untitled"
author: "Sergio Grueso"
date: "11/10/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<<<<<<< HEAD


# Index:
# - Intro
# - Method: 
#   1) Data analysis and visualization 
#   2) Moddeling Aproach
# - Results: summary of all the accuracies
# - Conclusions: 



# Download al load the libraries and data of the longitudinal and cross sectional studies (cross sectional study will only be used as a test set). All can be found in https://github.com/sgrueso/MRI.git.

```{r}
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(caret)) install.packages("caret")
if(!require(dslabs)) install.packages("dslabs")
if(!require(dbplyr)) install.packages("dbplyr")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(Rborist)) install.packages("Rborist")
if(!require(randomForest)) install.packages("randomForest")

set.seed(1, sample.kind = "Rounding")
longitudinal <- read.csv("../MRI/oasis_longitudinal.csv", header = TRUE, stringsAsFactors = FALSE)
```


# Summary of the longitudinal dataset

```{r}
str(data)

summary(longitudinal)
```


## Data analysis:

# Hand variable is not relevant because it has the same values for all subjects. Hand is droped.


```{r}
longitudinal$Hand <- NULL
```

# M.F variable name is changed to "Sex" to be more descriptive


```{r}
names(longitudinal)[names(longitudinal) == "M.F"] <- "Sex"
```


# Quick easy search for null values

```{r}
sum(is.na(longitudinal))
```

# 19 observations have one missing value from SES variable and 2 observations have missing values from SES and MMSE column. Subjects with 2 missing values are dropped and any missing value is replaced by the median of that variable. Median is used instead of mean because SES is a variable with integer values from 1 to 5 that indicates socioeconomical status and we do not want decimals values.

```{r}
ind <- which(is.na(longitudinal$MMSE))
longitudinal <- longitudinal[-ind, ]

ind <- which(is.na(longitudinal$SES))
longitudinal$SES[ind] <- median(longitudinal$SES[-ind])
```

# Group variable is the variable we will want to predict so demented is changed to 1 and nondemented to 0. This is in order to facilitate predictions, if the algorithm predicts anything greater than 0.5 (>0.5) the prediction will be "1" meaning that pacient has or is in risk of Alzheimer.

```{r}
longitudinal$Group <- ifelse(longitudinal$Group=="Demented", 1, 0)
longitudinal$Group <- as.factor(longitudinal$Group)
```

# correlations between variables

```{r}
longitudinal %>% select(MR.Delay, Age, EDUC, MMSE, CDR, eTIV, nWBV, ASF) %>% cor()
```

# (*** NO EN EDX ***) ASF is the computed scaling factor that transforms native-space brain and skull to the atlas target (i.e., the determinant of the transform matrix) and eTIV (estimated total intracranial volume in cm3) is the automated estimate of total intracranial volume in native space derived from the ASF (Buckner et al. 2004). That is why both variables are so highly correlated.

# Other significanly correlated (c < 0.20) variables are: 
#     - Age and MR.Delay
#     - Age and nWBV
#     - Education with eTIV and ASF
#     - nWBV with eTIV and ASF
#     - CDR and MMSE
# (*** NO EN EDX ***) 


## Data visualization
# Data visualization analysis can be started by observing the Group, Age and Sex distribution

```{r}
longitudinal %>% ggplot(aes(Sex, Age, fill= Group)) +
    geom_boxplot()
```

```{r}
longitudinal %>% ggplot(aes(Group, Age, fill = Group)) +
    geom_boxplot()
```


# Results on the test MMSE plotted against Age and colored depending on the Group.

```{r}
longitudinal %>% ggplot(aes(Age, MMSE, col = Group)) +
    geom_point()
```

# Comparison between Eductaion and Group

```{r}
longitudinal %>% ggplot(aes(Group, EDUC, fill = Group)) + geom_boxplot()
```


# SES vs Group

```{r}
longitudinal %>% ggplot(aes(Group, SES, fill = Group)) + geom_boxplot()
```


# As previous plots have shown, Education and MMSE have a negative correlation with Dementia. Therefore a final plot of MMSE against education is suggested with the expectation to find a positive correlation bewtween these 2 variables, and also, independently of the value in the variables Sex and Group.

```{r}
longitudinal %>% ggplot(aes(EDUC, MMSE, col = Group)) + geom_smooth(method = "lm", se = FALSE) + facet_wrap(~Sex)
```


# BUILDING THE MODEL

# Train and test set partitions are made:

```{r}
set.seed(1, sample.kind="Rounding")

test_index <- createDataPartition(y = longitudinal$Group, times = 1, p = 0.2, list = FALSE)
train_set <- longitudinal[-test_index,]
test_set  <- longitudinal[test_index,]
```

# Naive Baseline Model:

# The obvious way of predicting dementia seems to be MMSE. A value lower or equal than 27 predicts dementia.

```{r}
y_hat <- ifelse(test_set$MMSE <= 27, 1, 0)

mean(y_hat == test_set$Group)
```

# The model has a 0.84 accuracy with only one parameter, that is unsual and very high accuracy. But can I improve the accuracy if the model includes more parameters like Age, Education or Social Economic Status?. 

# MMSE is a test and includes scales that have into account Age and level of education of the subject. That is represented in our study in the variables Age and EDUC so the next model must include both variables.

```{r}
set.seed(1, sample.kind="Rounding")

model_1 <- train(Group ~ Age + EDUC + MMSE,
                 method = "glm",
                 data = train_set)

pred1 <- predict(model_1, test_set)

acc1 <- mean(pred1 == test_set$Group)

acc1
```

# The model is improved by adding both variables. Can more variables improve more the accurac?

```{r}
set.seed(1, sample.kind="Rounding")

model_2 <- train(Group ~ Age + EDUC + MMSE + SES + eTIV + nWBV,
                 method = "glm",
                 data = train_set)

pred2 <- predict(model_2, test_set)

acc2 <- mean(pred2 == test_set$Group)

acc2
```

# Indeed, more variables improve the model. 
# Other algorith are tested, for example Random Forest. We get a better accuracy than just MMSE model and glm.


```{r}
set.seed(1, sample.kind="Rounding")

train_rf <- train(Group ~ Age + EDUC + MMSE + SES + eTIV + nWBV,
               method = "rf",
               data = train_set)

pred_rf <- predict(train_rf, test_set)

acc_rf <- mean(pred_rf == test_set$Group)

acc_rf

# varImp(train_rf)

```



# Another algorithm is KNN but the accuracy is only 0.627 with k = 7 as best "k".

```{r}
set.seed(1, sample.kind="Rounding")

train_knn <- train(Group ~ MR.Delay + Age + EDUC + MMSE + SES + eTIV + nWBV,
               method = "knn",
               tuneGrid = data.frame(k = seq(1, 20, 1)),
               data = train_set)
confusionMatrix(predict(train_knn, test_set), test_set$Group)$overall["Accuracy"]

train_knn$bestTune
```











